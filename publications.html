<!doctype html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang=""> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8" lang=""> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9" lang=""> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang=""> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>AI Physics and Safety Lab</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="apple-touch-icon" href="apple-touch-icon.png">


        <!--Google fonts links-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet">


        <link rel="stylesheet" href="additional_assets/css/fonticons.css">
        <link rel="stylesheet" href="additional_assets/css/slider-pro.css">
        <!--<link rel="stylesheet" href="additional_assets/css/stylesheet.css">-->
        <link rel="stylesheet" href="additional_assets/css/font-awesome.min.css">
        <link rel="stylesheet" href="additional_assets/css/bootstrap.min.css">


        <!--For Plugins external css-->
        <link rel="stylesheet" href="additional_assets/css/plugins.css" />

        <!--Theme custom css -->
        <link rel="stylesheet" href="additional_assets/css/style.css">

        <!--Theme Responsive css-->
        <link rel="stylesheet" href="additional_assets/css/responsive.css" />

        <script src="additional_assets/js/vendor/modernizr-2.8.3-respond-1.4.2.min.js"></script>
		

		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/style.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		
		
    </head>
    <body data-spy="scroll" data-target=".navbar-collapse">
	   <nav>
		<ul class="sidebar">
		  <li onclick=hideSidebar()><a href="#"><svg xmlns="http://www.w3.org/2000/svg" height="26" viewBox="0 96 960 960" width="26"><path d="m249 849-42-42 231-231-231-231 42-42 231 231 231-231 42 42-231 231 231 231-42 42-231-231-231 231Z"/></svg></a></li>
		  <li><a href="index.html">Home</a></li>
		  <li><a href="research.html">Research</a></li>
		  <li><a href="team.html">Team</a></li>
			<li><a href="publications.html">Publications</a></li>
		  <li><a href="contact.html">Contact</a></li>
		</ul>
		<ul>
		  <li class="hideOnMobile"><a href="index.html">Home</a></li>
		  <li class="hideOnMobile"><a href="research.html">Research</a></li>
		  <li class="hideOnMobile"><a href="team.html">Team</a></li>
			<li  class="hideOnMobile"><a href="publications.html">Publications</a></li>
		  <li class="hideOnMobile"><a href="contact.html">Contact</a></li>
		  <li class="menu-button" onclick=showSidebar()><a href="#"><svg xmlns="http://www.w3.org/2000/svg" height="26" viewBox="0 96 960 960" width="26"><path d="M120 816v-60h720v60H120Zm0-210v-60h720v60H120Zm0-210v-60h720v60H120Z"/></svg></a></li>
		</ul>
	  </nav>

	  <script>
		function showSidebar(){
		  const sidebar = document.querySelector('.sidebar')
		  sidebar.style.display = 'flex'
		}
		function hideSidebar(){
		  const sidebar = document.querySelector('.sidebar')
		  sidebar.style.display = 'none'
		}
	  </script>

  <main class="content">
    <h2>   &nbsp;&nbsp;&nbsp;&nbsp;  Publications</h2>
    

    <section class="pub-list">

      
      <p> &nbsp;&nbsp;  - 01/2025: &nbsp;<em><strong>Gravity‑Bench‑v1: A Benchmark on Gravitational Physics Discovery for Agents.</strong></em>
        Accepted to ICML 2025 - arXiv:2501.18411.
        <a href="https://arxiv.org/abs/2501.18411">arXiv</a> ·
        <a href="https://gravitybench.github.io/">project page</a>
      </p>

		<p> &nbsp; Modern science emerged from reasoning over repeatedly-observed planetary motions. We present Gravity-Bench-v1, an environment-based benchmark that challenges AI agents on tasks that parallel this historical development. Gravity-Bench-v1 evaluates agents on the discovery of physics concealed within a dynamic environment, using rigorous gravitational dynamics simulations. Gravity-Bench includes out-of-distribution cases, i.e. with physics that deviates from the real world, to evaluate true scientific generalization capabilities. Agents must plan to collect data within an experimental budget and must perform a dynamic form of data analysis and reasoning to solve tasks efficiently. Our benchmark admits an open-ended space of solutions. Reference solutions for each task are provided to calibrate AI performance against human expertise. Technically at an upper-undergraduate level, our benchmark proves challenging to baseline AI agents. Gravity-Bench-v1 and planned extensions should help map out AI progress towards scientific discovery capabilities. </p>
      

      <p> &nbsp;&nbsp;  - 10/2024: &nbsp;<em><strong>Incremental AI Risks from Proxy-Simulations.</strong></em>
        <a href="https://hal.science/hal-04365629/document">HAL (open-access PDF)</a>
      </p>

		<p> &nbsp; Numerical simulations are versatile predictive tools that permit explorations of complex systems. The
ability of LLM agents to simulate real-world scenarios will expand the AI risk landscape. In the proxy-
simulation threat model, a user (or a deceptively aligned AI) can obfuscate the goal behind simulation-
based predictions by leveraging the generalizability of simulation tools. Three highly idealized proxy-
simulation examples are presented that illustrate how damage, casualties, and concealment of illegal
activities can be planned for, in obfuscation. This approach bypasses existing alignment and safety
filters (GPT4, Claude2 and LLama2). AI-enabled simulations facilitate access to prediction-based
planning that is not otherwise readily available. To the extent that goal obfuscation is possible, this
increases AI risk.</p>
            
      <p> &nbsp;&nbsp;  - 12/2023: &nbsp;<em><strong>Physics simulation capabilities of LLMs.</strong></em> Physica Scripta - 
        arXiv:2312.02091.
        <a href="https://arxiv.org/abs/2312.02091">arXiv</a>
      </p>

		<p> &nbsp; Large Language Models (LLMs) can solve some undergraduate-level to graduate-level physics textbook problems and are proficient at coding. Combining these two capabilities could one day enable AI systems to simulate and predict the physical world.
We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to research-level computational physics problems. We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains. We contribute   original and challenging problems in celestial mechanics (with REBOUND), stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear dynamics (with SciPy). Since our problems do not admit unique solutions, we evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more "educational" Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand.
As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems, although about 40\% of the solutions could plausibly get a passing grade. About   of the code lines produced are necessary, sufficient and correct (coding & physics). Physics and coding errors are the most common, with some unnecessary or insufficient lines. We observe significant variations across problem class and difficulty. We identify several failure modes of GPT4 in the computational physics domain.
Our reconnaissance work provides a snapshot of current computational capabilities in classical physics and points to obvious improvement targets if AI systems are ever to reach a basic level of autonomy in physics simulation capabilities. </p>

    </section>
  </main>

</body>
</html>
